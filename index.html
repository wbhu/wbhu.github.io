<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0">
    <title>HU Wenbo (&#x80E1;&#x6587;&#x535A;)</title>
    <meta content="Wenbo Hu, wbhu.github.io, 3D vision, video generation, depth estimation, NeRF, Gaussian Splatting"
        name="keywords">
    <meta name="description"
        content="Wenbo Hu's personal academic homepage - Senior Researcher at Tencent ARC Lab, Ph.D. from CUHK. Research on Generative World Models, 3D Vision, and Video Generation.">
    <meta name="author" content="Wenbo Hu">
    <meta name="theme-color" content="#1C8FE1">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://wbhu.github.io/">
    <meta property="og:title" content="HU Wenbo - Academic Homepage">
    <meta property="og:description"
        content="Senior Researcher at Tencent ARC Lab. Research on Generative World Models, 3D Vision, and Video Generation.">
    <meta property="og:image" content="https://wbhu.github.io/img/profile.jpeg">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@wbhu_cuhk">
    <meta name="twitter:title" content="HU Wenbo - Academic Homepage">
    <meta name="twitter:description"
        content="Senior Researcher at Tencent ARC Lab. Research on Generative World Models, 3D Vision, and Video Generation.">
    <meta name="twitter:image" content="https://wbhu.github.io/img/profile.jpeg">

    <!-- Canonical URL -->
    <link rel="canonical" href="https://wbhu.github.io/">

    <!-- Structured Data (JSON-LD) for better SEO -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "Person",
        "name": "Wenbo Hu",
        "alternateName": "胡文博",
        "url": "https://wbhu.github.io",
        "image": "https://wbhu.github.io/img/profile.jpeg",
        "jobTitle": "Senior Researcher",
        "worksFor": {
            "@type": "Organization",
            "name": "Tencent ARC Lab",
            "url": "https://ai.tencent.com"
        },
        "alumniOf": [
            {
                "@type": "CollegeOrUniversity",
                "name": "The Chinese University of Hong Kong",
                "url": "https://www.cuhk.edu.hk"
            },
            {
                "@type": "CollegeOrUniversity",
                "name": "Dalian University of Technology",
                "url": "https://www.dlut.edu.cn"
            }
        ],
        "knowsAbout": ["Generative World Models", "3D Vision", "Video Generation", "NeRF", "Gaussian Splatting", "Depth Estimation"],
        "sameAs": [
            "https://scholar.google.com/citations?hl=en&user=xDuDhwcAAAAJ",
            "https://github.com/wbhu",
            "https://x.com/wbhu_cuhk",
            "https://www.linkedin.com/in/huwenbo/",
            "https://www.youtube.com/channel/UCneF7qC0iCQFccoR3c7_hfw"
        ]
    }
    </script>

    <script src="jquery-latest.js"></script>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="icon" href="img/icon.svg">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
</head>

<body>

    <!--  TITLE  -->
    <div id="mainBlock">
        <div id="header">
            <div id="header-left">
                <img alt="HU Wenbo" src="img/profile.jpeg" />
            </div>
            <div id="header-content">
                <h1>
                    HU Wenbo (&#x80E1;&#x6587;&#x535A;)
                </h1>

                <h2>
                    Ph.D. @ CUHK,
                    <span class="spacing">Senior researcher @ Tencent ARC Lab</span>
                </h2>
                <div class="social-icons">
                    <a href="mailto:huwenbodut@gmail.com" class="icon-email" target="_blank" aria-label="Email"
                        title="Email"></a>
                    <a href="https://scholar.google.com/citations?hl=en&user=xDuDhwcAAAAJ" class="icon-scholar"
                        target="_blank" rel="noopener noreferrer" aria-label="Google Scholar"
                        title="Google Scholar"></a>
                    <a href="https://github.com/wbhu" class="icon-github" target="_blank" rel="noopener noreferrer"
                        aria-label="GitHub" title="GitHub"></a>
                    <a href="https://x.com/wbhu_cuhk" class="icon-x" target="_blank" rel="noopener noreferrer"
                        aria-label="X (Twitter)" title="X (Twitter)"></a>
                    <a href="https://www.youtube.com/channel/UCneF7qC0iCQFccoR3c7_hfw" class="icon-youtube"
                        target="_blank" rel="noopener noreferrer" aria-label="YouTube" title="YouTube"></a>
                    <a href="https://www.linkedin.com/in/huwenbo/" class="icon-linkedin" target="_blank"
                        rel="noopener noreferrer" aria-label="LinkedIn" title="LinkedIn"></a>
                    <a href="./img/wechat.jpeg" class="icon-wechat" target="_blank" aria-label="WeChat QR Code"
                        title="WeChat QR Code"></a>
                </div>
            </div>
        </div>

        <div id="content-wrapper">
            <!--  MENU  -->
            <div id="menu">
                <ul>
                    <a href="#" id="aboutButton">
                        <li>Home</li>
                    </a>
                    <a href="#" id="pubButton">
                        <li>Publications</li>
                    </a>
                    <a href="#" id="expButton">
                        <li>Experience</li>
                    </a>
                    <a href="#" id="patButton">
                        <li>Patents</li>
                    </a>
                    <a href="#" id="miscButton">
                        <li>Misc.</li>
                    </a>
                </ul>

                <div class="visitor-map">
                    <div class="visitor-map-hint" id="visitorMapHint"></div>
                    <a href="https://clustrmaps.com/site/1c2pj" title="ClustrMaps"><img
                            src="//www.clustrmaps.com/map_v2.png?d=EBYPWQy20NjXizST4__XUvNIbKpzqPE2D7NuYf0xQMw&cl=ffffff" />
                    </a>
                </div>
            </div>

            <div id="content-area">
                <!--  PROFILE  -->
                <div class="mainText" id="about">
                    <!--        <h3>Bio</h3>-->
                    <p align=left>

                        He was born in 1996 in Henan, China.
                        He graduated from Dalian University of Technology (DUT) in 2018 with a bachelor's degree in
                        computer
                        science
                        and technology (Outstanding Graduates of Liaoning Province), supervised by Prof. <a
                            href="https://xinyangdut.github.io/">YANG, Xin</a>.
                        He obtained his Ph.D. degree in Computer Science and Engineering from The Chinese University of
                        Hong
                        Kong
                        (CUHK) in 2022, supervised by Prof. <a href="https://ttwong12.github.io/myself.html">WONG,
                            Tien-Tsin</a>.
                        From 2022 to 2023, he was a researcher and developer at
                        <!--            <a href="https://www.picoxr.com/global">-->
                        PICO Mixed Reality, Bytedance.
                        <!--            </a>.-->
                        Since November 2023, he joined Tencent as a senior researcher, leading an effort to
                        <b>Generative World
                            Model</b>, including 3D from Images/Videos, Novel View Synthesis, and Video Generation.
                        <br><br>
                        His works have been selected as <b>Best Paper Finalist</b> in ICCV'2023, and <b>Best Paper</b>
                        at
                        PixFoundation workshop in CVPR'2025.
                        He received the <em>CCF Elite Collegiate Award</em> in 2017.
                        He has served as a reviewer for top-tier conferences and journals, including SIGGRAPH, SIGGRAPH
                        Asia,
                        CVPR,
                        ICCV, ECCV, NeurIPS, ICML, EG, TVCG, IJCV, etc.
                        <br><br>
                        <!--His research interests lie in the interface of Computer Graphics, Computer Vision, and AI, with a focus on 3D learning, including <b>Reconstruction, Rendering, Understanding, and Generation</b>.-->
                        <!--He is particularly interested in exploring how new-generation AI technologies benefit fundamental problems in computer vision and graphics.-->

                        <em class="highlight-hiring">
                            We have several open positions for <a
                                href="https://x.com/yshan2u/status/1897179915201732979">research interns and full-time
                                researchers</a>. Feel free to drop me an email if you are interested.
                        </em>
                    </p>


                    <h3 class="section-title">News!</h3>

                    <div class="news-scroll" style="max-height: 420px; overflow-y: auto; scrollbar-width: thin;">

                        <p align=left>
                            <b>[01/2026]</b>
                            Invited to give a talk at <em>Winter3DV</em>, a closed-door workshop on 3D vision.
                            <br />
                        </p>

                        <p align=left>
                            <b>[10/2025]</b>
                            Serving as Program Committee for <em>Mini3DV 2025</em> (a high-quality closed-door
                            workshop), on
                            World Modeling session.
                        </p>

                        <p align=left>
                            <b>[09/2025]</b>
                            <a href="https://drexubery.github.io/ViewCrafter">ViewCrafter</a> is accepted to
                            <em>TPAMI</em>!
                            <br />
                        </p>

                        <p align=left>
                            <b>[06/2025]</b>
                            Three papers (one is Oral) accepted to ICCV'25. <br />
                        </p>

                        <p align=left>
                            <b>[06/2025]</b>
                            Our DepthCrafter is selected as <span class="highlight-award">Best Paper</span> at
                            PixFoundation
                            workshop in CVPR'25!
                            <img src="img/bestpaper.jpg" alt="Best Paper Award"
                                style="max-width: 10%; margin-left: 10px; vertical-align: middle; display: inline-block;" />
                        </p>

                        <p align=left>
                            <b>[06/2025]</b>
                            Invited talk by <a href="https://mp.weixin.qq.com/s/6qsd9xAu8rP3n214Fkr6Xg">CSIG</a> about
                            "基于视频生成模型的场景演变生成最新进展".
                            <br />
                        </p>

                        <p align=left>
                            <b>[04/2025]</b>
                            Invited talk by <a href="https://mp.weixin.qq.com/s/SM4ZalAAuRweLGq5TiyKqQ">China3DV
                                2025</a> about
                            "GenConstruction".
                            <br />
                        </p>

                        <p align=left>
                            <b>[03/2025]</b>
                            Invited talk by <a href="https://mp.weixin.qq.com/s/wCaVMO0ZVHJx-BC_1hbHng">GAMES</a> about
                            "Generative Novel View Synthesis".
                            <br />
                        </p>

                        <p align=left>
                            <b>[03/2025]</b>
                            Invited talk by <a href="https://mp.weixin.qq.com/s/rjV969BoL6MzhQj1xR_ecQ">AnySyn3D</a>
                            about
                            "Video Diffusion for 3D".
                            <br />
                        </p>

                        <p align=left>
                            <b>[03/2025]</b>
                            We released code and models for <a
                                href="https://trajectorycrafter.github.io/">TrajectoryCrafter</a>.
                            <br />
                        </p>

                        <p align=left>
                            <b>[02/2025]</b>
                            Three papers accepted to CVPR'25. <br />
                        </p>

                        <p align=left>
                            <b>[10/2024]</b>
                            Invited talks by <a href="https://chinagraph2024.hsu.edu.cn/panels.html">Chinagraph</a>
                            about:
                            [<a href="https://depthcrafter.github.io/">DepthCrafter</a>, <a
                                href="https://stereocrafter.github.io/">StereoCrafter</a>,
                            <a href="https://drexubery.github.io/ViewCrafter">ViewCrafter</a>] and
                            [<a href="https://wbhu.github.io/projects/Tri-MipRF/index.html">Tri-MipRF</a>, <a
                                href="https://junchenliu77.github.io/Rip-NeRF/">Rip-NeRF</a>, <a
                                href="https://lzhnb.github.io/project-pages/analytic-splatting">Analytic-Splatting</a>].
                            <br>
                        </p>

                        <p align=left>
                            <b>[09/2024]</b>
                            One paper accepted to NeurIPS'24. <br />
                        </p>

                        <p align=left>
                            <b>[07/2024]</b>
                            Four papers accepted to ECCV'24. <br />
                        </p>

                        <p align=left>
                            <b>[05/2024]</b>
                            Invited talk by NeRF/GS & Beyond about <I>Anti-Aliasing in Neural Rendering</I>. <a
                                href="slides/AntiAliasingInNR_full.pdf">[Slides]</a><br />
                        </p>

                        <p align=left>
                            <b>[04/2024]</b>
                            Invited talk by China3DV 2024 about <I>Anti-Aliasing in Neural Rendering</I>. <br />
                        </p>

                        <p align=left>
                            <b>[03/2024]</b>
                            One paper conditionally accepted to SIGGRAPH'24. <br />
                        </p>

                        <p align=left>
                            <b>[02/2024]</b>
                            One paper accepted to CVPR'24. <br />
                        </p>

                        <p align=left>
                            <b>[10/2023]</b>
                            Our Tri-MipRF was selected in ICCV'23 <span class="highlight-award">Best Paper
                                Finalist</span>!
                            <br />
                        </p>

                        <p align=left>
                            <b>[08/2023]</b>
                            Invited talk by <a href="https://games-cn.org/games-webinar-20230831-290/">GAMES</a> about
                            our <a href="./projects/Tri-MipRF/index.html">Tri-MipRF</a>. <br />
                        </p>

                        <p align=left>
                            <b>[07/2023]</b>
                            Our Tri-MipRF was accepted to ICCV'23 as ORAL presentation. <br />
                        </p>

                        <p align=left>
                            <b>[07/2023]</b>
                            One paper on invertile image downscaling accepted to TIP'23. <br />
                        </p>

                        <p align=left>
                            <b>[08/2022]</b>
                            One paper conditionally accepted to SIGGRAPH Asia'22 (Journal Track). <br />
                        </p>

                        <p align=left>
                            <b>[05/2022]</b>
                            Passed the oral defense and became a Dr. <br />
                        </p>

                        <p align=left>
                            <b>[08/2021]</b>
                            Invited talk by <a href="https://course.zhidx.com/c/MTkzZGQxOGUwNzhiOGE5Njg4ZDM=">智东西</a>
                            about our <a href="./projects/BPNet/index.html">BPNet</a>.
                            <br />
                        </p>

                        <p align=left>
                            <b>[07/2021]</b>
                            One paper on lighting estimation accepted to TIP'22. <br />
                        </p>

                        <p align=left>
                            <b>[07/2021]</b>
                            Two papers accepted to ICCV'21. <br />
                        </p>

                        <p align=left>
                            <b>[07/2021]</b>
                            One paper on 3D human pose estimation accepted to ACM MM'21. <br />
                        </p>

                        <p align=left>
                            <b>[06/2021]</b>
                            Invited talk at <a href="http://www.scan-net.org/cvpr2021workshop">ScanNet CVPR'21
                                Workshop</a> about
                            our <a href="./projects/BPNet/index.html">BPNet</a> (<a
                                href="https://youtu.be/jr_bKmh6YUY?t=4141">Recording</a>).
                            <br />
                        </p>

                        <p align=left>
                            <b>[06/2021]</b>
                            Code of <a href="./projects/BPNet/index.html">BPNet</a> is released now.<br />
                        </p>

                        <p align=left>
                            <b>[03/2021]</b>
                            One paper conditionally accepted to CVPR 2021 (Oral). <br />
                        </p>

                        <p align=left>
                            <b>[08/2020]</b>
                            One paper conditionally accepted to SIGGRAPH Asia 2020. <br />
                        </p>

                        <p align=left>
                            <b>[08/2018]</b>
                            I start my Ph.D study at CUHK. <br />
                        </p>

                        <p align=left>
                            <b>[01/2018]</b>
                            I will work as a research intern at <a href="https://www.sensetime.com/">SenseTime</a>.
                        </p>

                    </div>

                </div>

                <!--  PUBLICATIONS  -->
                <div class="pubText" id="pub">

                    <div class="notation">
                        Selected publications. Full list on <a
                            href="https://scholar.google.com/citations?hl=en&user=xDuDhwcAAAAJ">Google Scholar</a>
                        <span class="notation-legend">
                            <sup>*</sup>equal contribution &nbsp;
                            <sup>&dagger;</sup>corresponding author &nbsp;
                            <sup>‡</sup>project leader
                        </span>
                    </div>

                    <hr class="line" />

                    <div class="pub-scroll" style="max-height: 850px; overflow-y: auto; scrollbar-width: thin;">

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/versecrafter_record.gif" /></div>
                            <div class="post-content">
                                <span class="paper-title">VerseCrafter: Dynamic Realistic Video World Model with 4D
                                    Geometric Control</span>
                                <div class="paper-authors">
                                    Sixiao Zheng, Minghao Yin, <b>Wenbo Hu<sup>&dagger;</sup></b>, Xiaoyu Li, Ying Shan,
                                    Yanwei Fu<sup>&dagger;</sup>
                                </div>
                                <div class="paper-venue">
                                    <em>arXiv preprint</em>, 2026
                                </div>
                                <div class="paper-links">
                                    <a href="https://sixiaozheng.github.io/VerseCrafter_page/">Project</a>
                                    <a href="https://arxiv.org/abs/2601.05138">Paper</a>
                                    <a href="https://github.com/TencentARC/VerseCrafter">Code</a>
                                    <a href="https://github.com/TencentARC/VerseCrafter" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/TencentARC/VerseCrafter?style=social"
                                            alt="GitHub stars"></a>
                                    <a href="https://huggingface.co/TencentARC/VerseCrafter">Model</a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/rollingforcing.gif" /></div>
                            <div class="post-content">
                                <span class="paper-title">Rolling Forcing: Autoregressive Long Video Diffusion in
                                    Real-Time</span>
                                <div class="paper-authors">
                                    Kunhao Liu, <b>Wenbo Hu<sup>&dagger;</sup></b>, Jiale Xu, Ying Shan, Shijian
                                    Lu<sup>&dagger;</sup>
                                </div>
                                <div class="paper-venue">
                                    <em>arXiv preprint</em>, 2025
                                </div>
                                <div class="paper-links">
                                    <a href="https://kunhao-liu.github.io/Rolling_Forcing_Webpage">Project</a>
                                    <a href="https://arxiv.org/abs/2509.25161">Paper</a>
                                    <a href="https://github.com/TencentARC/RollingForcing">Code</a>
                                    <a href="https://github.com/TencentARC/RollingForcing" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/TencentARC/RollingForcing?style=social"
                                            alt="GitHub stars"></a>
                                    <a href="https://huggingface.co/spaces/TencentARC/RollingForcing">Demo</a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/geometrycrafter.gif" /></div>
                            <div class="post-content">
                                <span class="paper-title">GeometryCrafter: Consistent Geometry Estimation for Open-world
                                    Videos with Diffusion Priors</span>
                                <div class="paper-authors">
                                    Tian-Xing Xu, Xiangjun Gao, <b>Wenbo Hu<sup>&dagger;</sup></b>, Xiaoyu Li, Song-Hai
                                    Zhang<sup>&dagger;</sup>, Ying Shan
                                </div>
                                <div class="paper-venue">
                                    <em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2025
                                </div>
                                <div class="paper-links">
                                    <a href="https://geometrycrafter.github.io/">Project</a>
                                    <a href="https://arxiv.org/abs/2504.01016">Paper</a>
                                    <a href="https://github.com/TencentARC/GeometryCrafter">Code</a>
                                    <a href="https://github.com/TencentARC/GeometryCrafter" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/TencentARC/GeometryCrafter?style=social"
                                            alt="GitHub stars"></a>
                                    <a href="https://huggingface.co/spaces/TencentARC/GeometryCrafter">Demo</a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/normalcrafter.gif" /></div>
                            <div class="post-content">
                                <span class="paper-title">NormalCrafter: Learning Temporally Consistent Normals from
                                    Video Diffusion Priors</span>
                                <div class="paper-authors">
                                    Yanrui Bin, <b>Wenbo Hu<sup>‡</sup></b>, Haoyuan Wang, Xinya Chen, Bing
                                    Wang<sup>&dagger;</sup>
                                </div>
                                <div class="paper-venue">
                                    <em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2025
                                </div>
                                <div class="paper-links">
                                    <a href="https://normalcrafter.github.io/">Project</a>
                                    <a href="https://arxiv.org/abs/2504.11427">Paper</a>
                                    <a href="https://github.com/Binyr/NormalCrafter">Code</a>
                                    <a href="https://github.com/Binyr/NormalCrafter" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/Binyr/NormalCrafter?style=social"
                                            alt="GitHub stars"></a>
                                    <a href="https://huggingface.co/spaces/Yanrui95/NormalCrafter">Demo</a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/TrajectoryCrafter.gif" /></div>
                            <div class="post-content">
                                <span class="paper-title">TrajectoryCrafter: Redirecting Camera Trajectory for Monocular
                                    Videos via Diffusion Models</span>
                                <div class="paper-authors">
                                    Mark YU, <b>Wenbo Hu<sup>&dagger;</sup></b>, Jinbo Xing, Ying Shan
                                </div>
                                <div class="paper-venue">
                                    <em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2025
                                    <span class="remark">Oral</span>
                                </div>
                                <div class="paper-links">
                                    <a href="https://trajectorycrafter.github.io/">Project</a>
                                    <a href="https://arxiv.org/pdf/2503.05638">Paper</a>
                                    <a href="https://github.com/TrajectoryCrafter/TrajectoryCrafter">Code</a>
                                    <a href="https://github.com/TrajectoryCrafter/TrajectoryCrafter"
                                        class="github-stars"><img
                                            src="https://img.shields.io/github/stars/TrajectoryCrafter/TrajectoryCrafter?style=social"
                                            alt="GitHub stars"></a>
                                    <a href="https://huggingface.co/spaces/Doubiiu/TrajectoryCrafter">Demo</a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/depthcrafter.gif" /></div>
                            <div class="post-content">
                                <span class="paper-title">DepthCrafter: Generating Consistent Long Depth Sequences for
                                    Open-world Videos</span>
                                <div class="paper-authors">
                                    <b>Wenbo Hu<sup>*&dagger;</sup></b>, Xiangjun Gao<sup>*</sup>, Xiaoyu
                                    Li<sup>*&dagger;</sup>, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan
                                </div>
                                <div class="paper-venue">
                                    <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>),
                                    2025
                                    <span class="remark">Highlight</span>
                                    <span class="remark">Best Paper @ PixFoundation</span>
                                </div>
                                <div class="paper-links">
                                    <a href="https://depthcrafter.github.io/">Project</a>
                                    <a href="https://arxiv.org/abs/2409.02095">Paper</a>
                                    <a href="https://github.com/wbhu/DepthCrafter">Code</a>
                                    <a href="https://github.com/wbhu/DepthCrafter" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/wbhu/DepthCrafter?style=social"
                                            alt="GitHub stars"></a>
                                    <a href="https://huggingface.co/spaces/tencent/DepthCrafter">Demo</a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/viewcrafter.gif" /></div>
                            <div class="post-content">
                                <span class="paper-title">ViewCrafter: Taming Video Diffusion Models for High-fidelity
                                    Novel View Synthesis</span>
                                <div class="paper-authors">
                                    Wangbo Yu<sup>*</sup>, Jinbo Xing<sup>*</sup>, Li Yuan<sup>*</sup>, <b>Wenbo
                                        Hu<sup>&dagger;</sup></b>, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin
                                    Wong, Ying Shan, Yonghong Tian<sup>&dagger;</sup>
                                </div>
                                <div class="paper-venue">
                                    <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>
                                    (<b>TPAMI</b>), 2025
                                </div>
                                <div class="paper-links">
                                    <a href="https://drexubery.github.io/ViewCrafter">Project</a>
                                    <a href="https://arxiv.org/abs/2409.02048">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=WGIEmu9eXmU">Video</a>
                                    <a href="https://github.com/Drexubery/ViewCrafter">Code</a>
                                    <a href="https://github.com/Drexubery/ViewCrafter" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/Drexubery/ViewCrafter?style=social"
                                            alt="GitHub stars"></a>
                                    <a href="https://huggingface.co/spaces/Doubiiu/ViewCrafter">Demo</a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/stereocrafter.png" /></div>
                            <div class="post-content">
                                <span class="paper-title">StereoCrafter: Diffusion-based Generation of Long and
                                    High-fidelity Stereoscopic 3D from Monocular Videos</span>
                                <div class="paper-authors">
                                    Sijie Zhao<sup>*</sup>, <b>Wenbo Hu<sup>*</sup></b>, Xiaodong Cun<sup>*</sup>, Yong
                                    Zhang<sup>&dagger;</sup>, Xiaoyu Li<sup>&dagger;</sup>, Zhe Kong, Xiangjun Gao,
                                    Muyao Niu, Ying Shan
                                </div>
                                <div class="paper-venue">
                                    <em>arXiv preprint</em>, 2024
                                </div>
                                <div class="paper-links">
                                    <a href="https://stereocrafter.github.io/">Project</a>
                                    <a href="https://arxiv.org/abs/2409.07447">Paper</a>
                                    <a href="https://github.com/TencentARC/StereoCrafter">Code</a>
                                    <a href="https://github.com/TencentARC/StereoCrafter" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/TencentARC/StereoCrafter?style=social"
                                            alt="GitHub stars"></a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/pixel-gs.png" /></div>
                            <div class="post-content">
                                <span class="paper-title">Pixel-GS: Density Control with Pixel-aware Gradient for 3D
                                    Gaussian Splatting</span>
                                <div class="paper-authors">
                                    Zheng Zhang, <b>Wenbo Hu<sup>&dagger;</sup></b>, Yixing Lao, Tong He, Hengshuang
                                    Zhao<sup>&dagger;</sup>
                                </div>
                                <div class="paper-venue">
                                    <em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2024
                                </div>
                                <div class="paper-links">
                                    <a href="https://pixelgs.github.io">Project</a>
                                    <a href="https://arxiv.org/pdf/2403.15530.pdf">Paper</a>
                                    <a href="https://github.com/zhengzhang01/Pixel-GS">Code</a>
                                    <a href="https://github.com/zhengzhang01/Pixel-GS" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/zhengzhang01/Pixel-GS?style=social"
                                            alt="GitHub stars"></a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/analytic-splatting.jpg" /></div>
                            <div class="post-content">
                                <span class="paper-title">Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via
                                    Analytic Integration</span>
                                <div class="paper-authors">
                                    Zhihao Liang, Qi Zhang, <b>Wenbo Hu</b>, Lei Zhu, Ying Feng, Kui Jia
                                </div>
                                <div class="paper-venue">
                                    <em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2024
                                    <span class="remark">Oral</span>
                                </div>
                                <div class="paper-links">
                                    <a href="https://lzhnb.github.io/project-pages/analytic-splatting">Project</a>
                                    <a href="https://arxiv.org/pdf/2403.11056.pdf">Paper</a>
                                    <a href="https://github.com/lzhnb/Analytic-Splatting">Code</a>
                                    <a href="https://github.com/lzhnb/Analytic-Splatting" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/lzhnb/Analytic-Splatting?style=social"
                                            alt="GitHub stars"></a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/texture-gs.jpg" /></div>
                            <div class="post-content">
                                <span class="paper-title">Texture-GS: Disentangling the Geometry and Texture for 3D
                                    Gaussian Splatting Editing</span>
                                <div class="paper-authors">
                                    Tian-Xing Xu, <b>Wenbo Hu<sup>&dagger;</sup></b>, Yu-Kun Lai, Ying Shan, Song-Hai
                                    Zhang<sup>&dagger;</sup>
                                </div>
                                <div class="paper-venue">
                                    <em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2024
                                </div>
                                <div class="paper-links">
                                    <a href="https://slothfulxtx.github.io/TexGS">Project</a>
                                    <a href="https://arxiv.org/abs/2403.10050">Paper</a>
                                    <a href="https://github.com/slothfulxtx/Texture-GS">Code</a>
                                    <a href="https://github.com/slothfulxtx/Texture-GS" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/slothfulxtx/Texture-GS?style=social"
                                            alt="GitHub stars"></a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/ripnerf.jpg" /></div>
                            <div class="post-content">
                                <span class="paper-title">Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded
                                    Platonic Solids</span>
                                <div class="paper-authors">
                                    Junchen Liu<sup>*</sup>, <b>Wenbo Hu<sup>*</sup></b>, Zhuo Yang<sup>*</sup>,
                                    Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao
                                </div>
                                <div class="paper-venue">
                                    <em>ACM SIGGRAPH</em>, 2024
                                </div>
                                <div class="paper-links">
                                    <a href="https://junchenliu77.github.io/Rip-NeRF/">Project</a>
                                    <a href="https://junchenliu77.github.io/Rip-NeRF/resources/Rip-NeRF.pdf">Paper</a>
                                    <a href="https://github.com/JunchenLiu77/Rip-NeRF">Code</a>
                                    <a href="https://github.com/JunchenLiu77/Rip-NeRF" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/JunchenLiu77/Rip-NeRF?style=social"
                                            alt="GitHub stars"></a>
                                    <a href="https://drive.google.com/file/d/1lmLJ7VN-Fbyohgdcrl7WMDj3gPk3WgUg">Data</a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/pbr-nerf.jpg" /></div>
                            <div class="post-content">
                                <span class="paper-title">Inverse Rendering of Glossy Objects via the Neural Plenoptic
                                    Function and Radiance Fields</span>
                                <div class="paper-authors">
                                    Haoyuan Wang, <b>Wenbo Hu<sup>&dagger;</sup></b>, Lei Zhu, Rynson W.H.
                                    Lau<sup>&dagger;</sup>
                                </div>
                                <div class="paper-venue">
                                    <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>),
                                    2024
                                </div>
                                <div class="paper-links">
                                    <a href="https://www.whyy.site/paper/nep">Project</a>
                                    <a href="pdf/PBR-NeRF.pdf">Paper</a>
                                    <a href="https://github.com/onpix/NeP">Code</a>
                                    <a href="https://github.com/onpix/NeP" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/onpix/NeP?style=social"
                                            alt="GitHub stars"></a>
                                    <a href="https://www.whyy.site/paper/nep">Data</a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/trimip.jpg" /></div>
                            <div class="post-content">
                                <span class="paper-title">Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing
                                    Neural Radiance Fields</span>
                                <div class="paper-authors">
                                    <b>Wenbo Hu</b>, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, Yuewen Ma
                                </div>
                                <div class="paper-venue">
                                    <em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2023
                                    <span class="remark">Oral</span>
                                    <span class="remark">Best Paper Finalist (17/8260)</span>
                                </div>
                                <div class="paper-links">
                                    <a href="projects/Tri-MipRF/index.html">Project</a>
                                    <a href="pdf/Tri-MipRF.pdf">Paper</a>
                                    <a href="https://github.com/wbhu/Tri-MipRF">Code</a>
                                    <a href="https://github.com/wbhu/Tri-MipRF" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/wbhu/Tri-MipRF?style=social"
                                            alt="GitHub stars"></a>
                                </div>
                            </div>
                        </div>


                        <div class="post-container">
                            <div class="post-thumb"><img src="img/aidn.gif" /></div>
                            <div class="post-content">
                                <span class="paper-title">Scale-arbitrary Invertible Image Downscaling</span>
                                <div class="paper-authors">
                                    Jinbo Xing<sup>*</sup>, <b>Wenbo Hu<sup>*</sup></b>, Menghan Xia, Tien-Tsin Wong
                                </div>
                                <div class="paper-venue">
                                    <em>IEEE Transactions on Image Processing</em> (<b>TIP</b>), 2023
                                </div>
                                <div class="paper-links">
                                    <a href="https://doubiiu.github.io/projects/aidn/">Project</a>
                                    <a href="https://arxiv.org/abs/2201.12576">Paper</a>
                                    <a href="https://github.com/Doubiiu/AIDN">Code</a>
                                    <a href="https://github.com/Doubiiu/AIDN" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/Doubiiu/AIDN?style=social"
                                            alt="GitHub stars"></a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/disColor.jpeg" /></div>
                            <div class="post-content">
                                <span class="paper-title">Disentangled Image Colorization via Global Anchors</span>
                                <div class="paper-authors">
                                    Menghan Xia, <b>Wenbo Hu</b>, Tien-Tsin Wong, Jue Wang
                                </div>
                                <div class="paper-venue">
                                    <em>ACM Transactions on Graphics</em> (<b>TOG</b>), 2022
                                    <span class="remark">SIGGRAPH Asia</span>
                                </div>
                                <div class="paper-links">
                                    <a href="https://menghanxia.github.io/projects/disco.html">Project</a>
                                    <a href="https://menghanxia.github.io/projects/disco/disco_main.pdf">Paper</a>
                                    <a href="https://github.com/MenghanXia/DisentangledColorization">Code</a>
                                    <a href="https://github.com/MenghanXia/DisentangledColorization"
                                        class="github-stars"><img
                                            src="https://img.shields.io/github/stars/MenghanXia/DisentangledColorization?style=social"
                                            alt="GitHub stars"></a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/invHalftone.jpeg" /></div>
                            <div class="post-content">
                                <span class="paper-title">Deep Halftoning with Reversible Binary Pattern</span>
                                <div class="paper-authors">
                                    Menghan Xia, <b>Wenbo Hu</b>, Xueting Liu, Tien-Tsin Wong
                                </div>
                                <div class="paper-venue">
                                    <em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2021
                                </div>
                                <div class="paper-links">
                                    <a
                                        href="https://www.cse.cuhk.edu.hk/~ttwong/papers/invhalftone/invhalftone.html">Project</a>
                                    <a
                                        href="http://appsrv.cse.cuhk.edu.hk/~ttwong/cgi-bin/paper-download/download.cgi?path=invhalftone&dl=invhalftone.pdf">Paper</a>
                                    <a href="https://github.com/MenghanXia/ReversibleHalftoning">Code</a>
                                    <a href="https://github.com/MenghanXia/ReversibleHalftoning"
                                        class="github-stars"><img
                                            src="https://img.shields.io/github/stars/MenghanXia/ReversibleHalftoning?style=social"
                                            alt="GitHub stars"></a>
                                </div>
                            </div>
                        </div>

                        <div class="post-container">
                            <div class="post-thumb"><img src="img/CondDGConv.jpg" /></div>
                            <div class="post-content">
                                <span class="paper-title">Conditional Directed Graph Convolution for 3D Human Pose
                                    Estimation</span>
                                <div class="paper-authors">
                                    <b>Wenbo Hu</b>, Changgong Zhang, Fangneng Zhan, Lei Zhang, Tien-Tsin Wong
                                </div>
                                <div class="paper-venue">
                                    <em>ACM International Conference on Multimedia</em> (<b>ACM MM</b>), 2021
                                </div>
                                <div class="paper-links">
                                    <a href="projects/CondDGConv/index.html">Project</a>
                                    <a href="./pdf/CondDGConv_update.pdf">Paper</a>
                                </div>
                            </div>
                        </div>


                        <div class="post-container">
                            <div class="post-thumb"><img src="projects/BPNet/img/bpnet.jpg" /></div>
                            <div class="post-content">
                                <span class="paper-title">Bidirectional Projection Network for Cross Dimensional Scene
                                    Understanding</span>
                                <div class="paper-authors">
                                    <b>Wenbo Hu</b>, Hengshuang Zhao, Li Jiang, Jiaya Jia, Tien-Tsin Wong
                                </div>
                                <div class="paper-venue">
                                    <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>),
                                    2021
                                    <span class="remark">Oral</span>
                                </div>
                                <div class="paper-links">
                                    <a href="projects/BPNet/index.html">Project</a>
                                    <a href="https://arxiv.org/abs/2103.14326">Paper</a>
                                    <a href="https://github.com/wbhu/BPNet">Code</a>
                                    <a href="https://github.com/wbhu/BPNet" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/wbhu/BPNet?style=social"
                                            alt="GitHub stars"></a>
                                </div>
                            </div>
                        </div>


                        <div class="post-container">
                            <div class="post-thumb"><img src="projects/Mono3D/img/teaser.jpg" /></div>
                            <div class="post-content">
                                <span class="paper-title">Mononizing Binocular Videos</span>
                                <div class="paper-authors">
                                    <b>Wenbo Hu</b>, Menghan Xia, Chi-Wing Fu, Tien-Tsin Wong
                                </div>
                                <div class="paper-venue">
                                    <em>ACM Transactions on Graphics</em> (<b>TOG</b>), 2020
                                    <span class="remark">SIGGRAPH Asia</span>
                                </div>
                                <div class="paper-links">
                                    <a href="projects/Mono3D/index.html">Project</a>
                                    <a href="https://arxiv.org/abs/2009.01424">Paper</a>
                                    <a href="https://github.com/wbhu/Mono3D">Code</a>
                                    <a href="https://github.com/wbhu/Mono3D" class="github-stars"><img
                                            src="https://img.shields.io/github/stars/wbhu/Mono3D?style=social"
                                            alt="GitHub stars"></a>
                                </div>
                            </div>
                        </div>

                        <!--        <div class="post-container">-->
                        <!--            <div class="post-thumb"><img-->
                        <!--                    src="img/vec.jpg"-->
                        <!--                    width="380" height="140"/>-->
                        <!--            </div>-->
                        <!--            <div class="post-content">-->
                        <!--                <b>Deep Line Drawing Vectorization via Line Subdivision and Topology Reconstruction</b>-->
                        <!--                <br>-->
                        <!--                &nbsp-->
                        <!--                Yi Guo, Zhuming Zhang, Chu Han, <I>Wenbo Hu,</I> Chengze Li, Tien-Tsin Wong-->
                        <!--                <br>-->
                        <!--                <I>Computer Graphics Forum (CGF), <b>Pacific Graphics (PG),</b> 2019</I>-->
                        <!--                <br>-->
                        <!--                &nbsp-->
                        <!--                [<a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/cgf.13818"> Paper</a>]-->
                        <!--            </div>-->
                        <!--        </div>-->

                        <!--        <div class="post-container">-->
                        <!--            <div class="post-thumb"><img-->
                        <!--                    src="img/DEMC.jpg"-->
                        <!--                    width="380" height="140"/>-->
                        <!--            </div>-->
                        <!--            <div class="post-content">-->
                        <!--                <b>DEMC: A Deep Dual-Encoder Network for Denoising Monte Carlo Rendering</b>-->
                        <!--                <br>-->
                        <!--                &nbsp Xin Yang, <I>Wenbo Hu,</I> Dawei Wang, Lijing Zhao, Baocai Yin, Qiang Zhang,-->
                        <!--                <br>-->
                        <!--                &nbsp-->
                        <!--                Xiaopeng Wei, Hongbo Fu-->
                        <!--                <br>-->
                        <!--                <em>Computational Visual Media,</em><b>CVM,</b>2019.-->
                        <!--                </br>-->
                        <!--                &nbsp-->
                        <!--                [<a href="https://sites.google.com/view/dutdemc"> Project</a>]-->
                        <!--                [<a href="https://arxiv.org/pdf/1905.03908.pdf"> Paper</a>]-->
                        <!--                &lt;!&ndash;                [<a href="bib/DEMC.bib"> Bib</a> ]&ndash;&gt;-->
                        <!--                [<a href="https://drive.google.com/drive/folders/1sBDN5vHykSB-YNU_pvzzV1u5WLLgw7X-?usp=sharing">-->
                        <!--                Dataset</a> ]-->
                        <!--                [<a href="https://github.com/wbhu/DEMC"> Code</a> ]-->

                        <!--            </div>-->
                        <!--        </div>-->

                        <!--        <div class="post-container">-->
                        <!--            <div class="post-thumb"><img-->
                        <!--                    src="img/FRMC.gif"-->
                        <!--                    width="380" height="140"/>-->
                        <!--            </div>-->
                        <!--            <div class="post-content">-->
                        <!--                <b>Fast Reconstruction for Monte Carlo Rendering Using Deep Convolutional Networks</b>-->
                        <!--                <br>-->
                        <!--                &nbsp-->
                        <!--                Xin Yang, Dawei Wang, <I>Wenbo Hu,</I> Lijing Zhao, Xinglin Piao, Dongsheng Zhou,-->
                        <!--                <br>-->
                        <!--                &nbsp-->
                        <!--                Qiang Zhang, Baocai Yin, Qiang Cai, Xiaopeng Wei-->
                        <!--                <br>-->
                        <!--                <I>IEEE Access, 2018</I>-->
                        <!--                <br>-->
                        <!--                &nbsp-->
                        <!--                [<a href="https://ieeexplore.ieee.org/abstract/document/8571234"> Paper</a>]-->
                        <!--                &lt;!&ndash;                [<a href="bib/FRMC.bib"> Bib</a> ]&ndash;&gt;-->
                        <!--            </div>-->
                        <!--        </div>-->
                    </div>


                </div>

                <!--  EXPERIENCE  -->
                <div class="mainText" id="exp">
                    <h3>Education</h3>
                    <hr class="line" />

                    <div class="exp-container">
                        <div class="exp-logo"><img src="img/cuhk_logo.jpeg" alt="CUHK" /></div>
                        <div class="exp-content">
                            <div class="exp-period">Aug. 2018 - May. 2022</div>
                            <div class="exp-org">The Chinese University of Hong Kong</div>
                            <div class="exp-dept">Dept. of Computer Science & Engineering</div>
                            <div class="exp-role">Ph.D. in Computer Science</div>
                        </div>
                    </div>

                    <div class="exp-container">
                        <div class="exp-logo"><img src="img/dut_long.jpeg" alt="DUT" /></div>
                        <div class="exp-content">
                            <div class="exp-period">Sep. 2014 - Jun. 2018</div>
                            <div class="exp-org">Dalian University of Technology</div>
                            <div class="exp-dept">Dept. of Computer Science and Technology</div>
                            <div class="exp-role">B.Eng. in Computer Science <span class="highlight">Rank: 3/105, GPA:
                                    90.3/100</span></div>
                        </div>
                    </div>

                    <div class="section-divider"></div>
                    <h3>Work Experience</h3>
                    <hr class="line" />

                    <div class="exp-container">
                        <div class="exp-logo"><img src="img/TencentAILab.jpeg" alt="Tencent" /></div>
                        <div class="exp-content">
                            <div class="exp-period">Dec. 2023 - Present</div>
                            <div class="exp-org"><a href="https://ai.tencent.com">Tencent AI Lab (ARC)</a></div>
                            <div class="exp-role">Senior Researcher</div>
                        </div>
                    </div>

                    <div class="exp-container">
                        <div class="exp-logo"><img src="img/PICO.jpg" alt="PICO" /></div>
                        <div class="exp-content">
                            <div class="exp-period">Sep. 2022 - Nov. 2023</div>
                            <div class="exp-org"><a href="https://www.picoxr.com/global">PICO Mixed Reality,
                                    ByteDance</a></div>
                            <div class="exp-role">Researcher & Developer</div>
                        </div>
                    </div>

                    <div class="exp-container">
                        <div class="exp-logo"><img src="img/TencentAILab.jpeg" alt="Tencent" /></div>
                        <div class="exp-content">
                            <div class="exp-period">Nov. 2021 - May. 2022</div>
                            <div class="exp-org"><a href="https://ai.tencent.com">Tencent AI Lab</a></div>
                            <div class="exp-role">Research Intern</div>
                        </div>
                    </div>

                    <div class="exp-container">
                        <div class="exp-logo"><img src="img/DAMO.jpeg" alt="DAMO" /></div>
                        <div class="exp-content">
                            <div class="exp-period">Sep. 2020 - Nov. 2021</div>
                            <div class="exp-org"><a href="https://damo.alibaba.com/">DAMO Academy, Alibaba Group</a>
                            </div>
                            <div class="exp-role">Research Intern</div>
                        </div>
                    </div>

                    <div class="exp-container">
                        <div class="exp-logo"><img src="img/sensetime.jpg" alt="SenseTime" /></div>
                        <div class="exp-content">
                            <div class="exp-period">Jan. 2018 - Jul. 2018</div>
                            <div class="exp-org"><a href="https://www.sensetime.com/">SenseTime</a></div>
                            <div class="exp-role">Research Intern</div>
                        </div>
                    </div>

                    <div class="exp-container">
                        <div class="exp-logo"><img src="img/dji.jpg" alt="DJI" /></div>
                        <div class="exp-content">
                            <div class="exp-period">Jul. 2016 - Sep. 2016</div>
                            <div class="exp-org"><a href="https://www.dji.com/">DJI</a></div>
                            <div class="exp-role">Software Engineer Intern</div>
                        </div>
                    </div>

                </div>

                <!--  PATENTS  -->
                <div class="mainText" id="pat">


                    <ul>
                        <li>CN108827302A (In process), “Multi-rotor aerocraft air navigation aid based on rotor
                            tachometric
                            survey”.
                        </li>
                        <li>CN107656227B (Issued Oct. 2019), “Magnetometer calibration method based on
                            Levenberg-Marquardt
                            algorithm”.
                        </li>
                        <li>CN107655463B (Issued Oct. 2019), “Electronic compass calibration method based on simulated
                            annealing”.
                        </li>
                        <li>CN207400517U (Issued May. 2018), “Intelligent toothbrush system ”.</li>
                    </ul>
                </div>

                <!--  Miscellaneous -->
                <div class="mainText" id="mis">
                    <h3>Miscellaneous</h3>
                    <hr class="line" />

                    <!--<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share our <a-->
                    <!--        href="https://twitter.com/hashtag/TrajectoryCrafter?src=hash&amp;ref_src=twsrc%5Etfw">#TrajectoryCrafter</a>,-->
                    <!--    a diffusion model for Redirecting Camera Trajectory in Monocular Videos! <br><br>Try to explore the world-->
                    <!--    underlying your videos~<br><br>Page: <a href="https://t.co/hWuDRDcv10">https://t.co/hWuDRDcv10</a><br>Demo:-->
                    <!--    <a href="https://t.co/e3JF0SSXIC">https://t.co/e3JF0SSXIC</a><br>Code: <a href="https://t.co/Y84MN6D2iO">https://t.co/Y84MN6D2iO</a>-->
                    <!--    <a href="https://t.co/WAS9Vbbosu">pic.twitter.com/WAS9Vbbosu</a></p>&mdash; HU, Wenbo (@wbhu_cuhk) <a-->
                    <!--        href="https://twitter.com/wbhu_cuhk/status/1898988354143674423?ref_src=twsrc%5Etfw">March 10, 2025</a>-->
                    <!--</blockquote>-->
                    <!--<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>-->

                    <!--<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share our DepthCrafter, a super consistent-->
                    <!--    video depth model for long open-world videos!<br><br>Project webpage: <a href="https://t.co/9SiMUv4hoW">https://t.co/9SiMUv4hoW</a>-->
                    <!--    <a href="https://t.co/qy55L7cm44">https://t.co/qy55L7cm44</a> <a href="https://t.co/TZBWnjEGmk">pic.twitter.com/TZBWnjEGmk</a>-->
                    <!--</p>&mdash; HU, Wenbo (@wbhu_cuhk) <a-->
                    <!--        href="https://twitter.com/wbhu_cuhk/status/1831227896331112591?ref_src=twsrc%5Etfw">September 4,-->
                    <!--    2024</a></blockquote>-->
                    <!--<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>-->

                    <div class="tweet-container">

                        <div class="tweet-item">
                            <blockquote class="twitter-tweet">
                                <p lang="en" dir="ltr">Excited to share our <a
                                        href="https://twitter.com/hashtag/TrajectoryCrafter?src=hash&amp;ref_src=twsrc%5Etfw">#TrajectoryCrafter</a>,
                                    a diffusion model for Redirecting Camera Trajectory in Monocular Videos! <br><br>Try
                                    to
                                    explore the
                                    world
                                    underlying your videos~<br><br>Page: <a
                                        href="https://t.co/hWuDRDcv10">https://t.co/hWuDRDcv10</a><br>Demo:
                                    <a href="https://t.co/e3JF0SSXIC">https://t.co/e3JF0SSXIC</a><br>Code: <a
                                        href="https://t.co/Y84MN6D2iO">https://t.co/Y84MN6D2iO</a>
                                    <a href="https://t.co/WAS9Vbbosu">pic.twitter.com/WAS9Vbbosu</a>
                                </p>&mdash; HU, Wenbo (@wbhu_cuhk)
                                <a href="https://twitter.com/wbhu_cuhk/status/1898988354143674423?ref_src=twsrc%5Etfw">March
                                    10, 2025</a>
                            </blockquote>
                            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                        </div>

                        <div class="tweet-item">
                            <blockquote class="twitter-tweet">
                                <p lang="en" dir="ltr">Introducing 𝚅𝚒𝚎𝚠𝙲𝚛𝚊𝚏𝚝𝚎𝚛 🥳.
                                    𝚅𝚒𝚎𝚠𝙲𝚛𝚊𝚏𝚝𝚎𝚛 can generate high-fidelity novel views from single or sparse
                                    input
                                    images with accurate
                                    camera pose control!<br>✨Paper: <a
                                        href="https://t.co/dH4Dw0Eb1e">https://t.co/dH4Dw0Eb1e</a><br>🎯Code:
                                    <a href="https://t.co/53ai21Px99">https://t.co/53ai21Px99</a><br>🥁Demo: <a
                                        href="https://t.co/9xCyqtsFco">https://t.co/9xCyqtsFco</a> <a
                                        href="https://t.co/R5ZZpYfdM3">pic.twitter.com/R5ZZpYfdM3</a>
                                </p>&mdash; Jinbo Xing
                                (@Double47685693) <a
                                    href="https://twitter.com/Double47685693/status/1831517092606308703?ref_src=twsrc%5Etfw">September
                                    5, 2024</a>
                            </blockquote>
                            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                        </div>

                        <div class="tweet-item">
                            <blockquote class="twitter-tweet">
                                <p lang="en" dir="ltr">I ported DepthCrafter to ComfyUI! 🔥<br><br>Now
                                    you can generate super stable depthmap videos from any input video.<br><br>The VRAM
                                    requirement is
                                    pretty high (&gt;16GB) if you want to render long videos in high res (768p and
                                    up)<br><br>It
                                    pairs
                                    well with Depthflow!<br><br>Repo link in comments below👇 <a
                                        href="https://t.co/dlS5wr7mRU">pic.twitter.com/dlS5wr7mRU</a>
                                </p>&mdash; akatz (@akatz_ai) <a
                                    href="https://twitter.com/akatz_ai/status/1847374687384527209?ref_src=twsrc%5Etfw">October
                                    18,
                                    2024</a>
                            </blockquote>
                            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                        </div>

                        <div class="tweet-item">
                            <blockquote class="twitter-tweet">
                                <p lang="en" dir="ltr">Introducing StereoCrafter: Transforming
                                    monocular videos into high-fidelity 3D movies, compatible with various depth
                                    estimation
                                    methods and
                                    currently performing best with DepthCrafter. Feel free to download and experience
                                    the 3D
                                    results on
                                    our project page using Vision Pros,… <a
                                        href="https://t.co/6YEHisStz8">pic.twitter.com/6YEHisStz8</a></p>&mdash; Ying
                                Shan
                                (@yshan2u) <a
                                    href="https://twitter.com/yshan2u/status/1834126260341277024?ref_src=twsrc%5Etfw">September
                                    12, 2024</a>
                            </blockquote>
                            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                        </div>

                        <div class="tweet-item">
                            <blockquote class="twitter-tweet">
                                <p lang="en" dir="ltr">Excited to share our DepthCrafter, a super
                                    consistent
                                    video depth model for long open-world videos!<br><br>Project webpage: <a
                                        href="https://t.co/9SiMUv4hoW">https://t.co/9SiMUv4hoW</a>
                                    <a href="https://t.co/qy55L7cm44">https://t.co/qy55L7cm44</a> <a
                                        href="https://t.co/TZBWnjEGmk">pic.twitter.com/TZBWnjEGmk</a>
                                </p>&mdash; HU, Wenbo (@wbhu_cuhk) <a
                                    href="https://twitter.com/wbhu_cuhk/status/1831227896331112591?ref_src=twsrc%5Etfw">September
                                    4,
                                    2024</a>
                            </blockquote>
                            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                        </div>

                        <div class="tweet-item">
                            <blockquote class="twitter-tweet">
                                <p lang="zxx" dir="ltr"><a href="https://t.co/ZzpcYAHw6I">pic.twitter.com/ZzpcYAHw6I</a>
                                </p>&mdash; Kosta Derpanis (@CSProfKGD) <a
                                    href="https://twitter.com/CSProfKGD/status/1709468280371466453?ref_src=twsrc%5Etfw">October
                                    4,
                                    2023</a>
                            </blockquote>
                            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                        </div>

                    </div>

                </div>
            </div>
        </div>
    </div>

    <script type="text/javascript" src="./script.js"></script>

</body>

</html>