<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>HU Wenbo (&#x80E1;&#x6587;&#x535A;)</title>
    <meta content="Wenbo Hu, wbhu.github.io" name="keywords">
    <script src="jquery-latest.js"></script>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="icon" href="img/icon.svg">
</head>

<body>

<!--  TITLE  -->
<div id="mainBlock">
    <div id="header">
        <img align="left" alt="HU Wenbo" src="img/profile.jpeg" width="180"/>

        <h1>
            <br>
            HU Wenbo (&#x80E1;&#x6587;&#x535A;)
        </h1>

        <h2>
            Ph.D. @ CUHK,
            <span class="spacing">Senior researcher @ Tencent ARC Lab</span>
            <br>
            <br>
            <a href="mailto:huwenbodut@gmail.com" class="icon-email" target="_blank"></a>
            <a href="https://scholar.google.com/citations?hl=en&user=xDuDhwcAAAAJ" class="icon-scholar"
               target="_blank"></a>
            <a href="https://github.com/wbhu" class="icon-github" target="_blank"></a>
            <a href="https://x.com/wbhu_cuhk" class="icon-twitter" target="_blank"></a>
            <a href="https://www.youtube.com/channel/UCneF7qC0iCQFccoR3c7_hfw" class="icon-youtube"
               target="_blank"></a>
            <a href="https://www.linkedin.com/in/huwenbo/" class="icon-linkedin" target="_blank"></a>
            <!--            </p>-->
            <a href="./img/wechat.jpeg" class="icon-wechat" target="_blank"></a>
        </h2>
    </div>

    <!--  MENU  -->
    <div id="menu">
        <ul>
            <a href="#" id="aboutButton">
                <li>Home</li>
            </a>
            <a href="#" id="pubButton">
                <li>Publications</li>
            </a>
            <a href="#" id="expButton">
                <li>Experience</li>
            </a>
            <a href="#" id="patButton">
                <li>Patents</li>
            </a>
            <a href="#" id="miscButton">
                <li>Miscellaneous</li>
            </a>
        </ul>

        <br>
        <div align="left" style="width: 100px; height: 100px;">
            <!--            <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=0h7kmkjhde1&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=1&amp;s=170" async="async"></script>-->
            <script type="text/javascript" id="clstr_globe"
                    src="//cdn.clustrmaps.com/globe.js?d=EBYPWQy20NjXizST4__XUvNIbKpzqPE2D7NuYf0xQMw"></script>
        </div>
    </div>

    <!--  PROFILE  -->
    <div class="mainText" id="about">
        <!--        <h3>Bio</h3>-->
        <p align=left>

            He was born in 1996 in Henan, China.
            He graduated from Dalian University of Technology (DUT) in 2018 with a bachelor's degree in computer science
            and technology (Outstanding Graduates of Liaoning Province), supervised by Prof. <a
                href="https://xinyangdut.github.io/">YANG, Xin</a>.
            He obtained his Ph.D. degree in Computer Science and Engineering from The Chinese University of Hong Kong
            (CUHK) in 2022, supervised by Prof. <a href="https://ttwong12.github.io/myself.html">WONG, Tien-Tsin</a>.
            From 2022 to 2023, he was a researcher and developer at
            <!--            <a href="https://www.picoxr.com/global">-->
            PICO Mixed Reality, Bytedance.
            <!--            </a>.-->
            Since November 2023, he joined Tencent as a senior researcher, leading an effort to <b>Generative World
            Model</b>, including 3D from Images/Videos, Novel View Synthesis, and Video Generation.
            <br><br>
            His works have been selected as <b>Best Paper Finalist</b> in ICCV'2023, and <b>Best Paper</b> at PixFoundation workshop in CVPR'2025.
            He received the <em>CCF Elite Collegiate Award</em> in 2017.
            He has served as a reviewer for top-tier conferences and journals, including SIGGRAPH, SIGGRAPH Asia, CVPR,
            ICCV, ECCV, NeurIPS, ICML, EG, TVCG, IJCV, etc.
            <br><br>
            <!--His research interests lie in the interface of Computer Graphics, Computer Vision, and AI, with a focus on 3D learning, including <b>Reconstruction, Rendering, Understanding, and Generation</b>.-->
            <!--He is particularly interested in exploring how new-generation AI technologies benefit fundamental problems in computer vision and graphics.-->

            <em><span style=color:#d3610f>
                <!--If you'd like to collaborate with me for publishing top-tier papers, please feel free to drop me an email.-->
                We have several open positions for <a href="https://x.com/yshan2u/status/1897179915201732979"> research interns and full-time researchers</a>. Feel free to drop me an email if you are interested.
            </span></em>
        </p>


        <p align=left>
            <span style="font-size: large; "><em><b><span style=color:red>News!</span><br/></b></em></span>
        </p>

        <div style="max-height: 420px; overflow-y: auto; background-color: #f0f0f0; scrollbar-width: thin;">

            <p align=left>
                <b>[09/2025]</b>
                <a href="https://drexubery.github.io/ViewCrafter">ViewCrafter</a> is accepted to <em>TPAMI</em>!
                <br/>
            </p>

            <p align=left>
                <b>[06/2025]</b>
                Three papers accepted to ICCV'25. <br/>
            </p>

            <p align=left>
                <b>[06/2025]</b>
                Our DepthCrafter is selected as <span style=color:sandybrown>Best Paper</span> at PixFoundation workshop in CVPR'25!
                <img src="img/bestpaper.jpg" alt="Best Paper Award" style="max-width: 10%; margin-left: 10px; vertical-align: middle; display: inline-block;"/>
            </p>

            <p align=left>
                <b>[06/2025]</b>
                Invited talk by <a href="https://mp.weixin.qq.com/s/6qsd9xAu8rP3n214Fkr6Xg">CSIG</a> about "Âü∫‰∫éËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑÂú∫ÊôØÊºîÂèòÁîüÊàêÊúÄÊñ∞ËøõÂ±ï".
                <br/>
            </p>

            <p align=left>
                <b>[04/2025]</b>
                Invited talk by <a href="https://mp.weixin.qq.com/s/SM4ZalAAuRweLGq5TiyKqQ">China3DV 2025</a> about "GenConstruction".
                <br/>
            </p>

            <p align=left>
                <b>[03/2025]</b>
                Invited talk by <a href="https://mp.weixin.qq.com/s/wCaVMO0ZVHJx-BC_1hbHng">GAMES</a> about "Generative Novel View Synthesis".
                <br/>
            </p>

            <p align=left>
                <b>[03/2025]</b>
                Invited talk by <a href="https://mp.weixin.qq.com/s/rjV969BoL6MzhQj1xR_ecQ">AnySyn3D</a> about "Video Diffusion for 3D".
                <br/>
            </p>

            <p align=left>
                <b>[03/2025]</b>
                We released code and models for <a href="https://trajectorycrafter.github.io/">TrajectoryCrafter</a>.
                <br/>
            </p>

            <p align=left>
                <b>[02/2025]</b>
                Three papers accepted to CVPR'25. <br/>
            </p>

            <p align=left>
                <b>[10/2024]</b>
                Invited talks by <a href="https://chinagraph2024.hsu.edu.cn/panels.html">Chinagraph</a> about:
                [<a href="https://depthcrafter.github.io/">DepthCrafter</a>, <a href="https://stereocrafter.github.io/">StereoCrafter</a>,
                <a href="https://drexubery.github.io/ViewCrafter">ViewCrafter</a>] and
                [<a href="https://wbhu.github.io/projects/Tri-MipRF/index.html">Tri-MipRF</a>, <a
                    href="https://junchenliu77.github.io/Rip-NeRF/">Rip-NeRF</a>, <a
                    href="https://lzhnb.github.io/project-pages/analytic-splatting">Analytic-Splatting</a>].
                <br>
            </p>

            <p align=left>
                <b>[09/2024]</b>
                One paper accepted to NeurIPS'24. <br/>
            </p>

            <p align=left>
                <b>[07/2024]</b>
                Four papers accepted to ECCV'24. <br/>
            </p>

            <p align=left>
                <b>[05/2024]</b>
                Invited talk by NeRF/GS & Beyond about <I>Anti-Aliasing in Neural Rendering</I>. <a
                    href="slides/AntiAliasingInNR_full.pdf">[Slides]</a><br/>
            </p>

            <p align=left>
                <b>[04/2024]</b>
                Invited talk by China3DV 2024 about <I>Anti-Aliasing in Neural Rendering</I>. <br/>
            </p>

            <p align=left>
                <b>[03/2024]</b>
                One paper conditionally accepted to SIGGRAPH'24. <br/>
            </p>

            <p align=left>
                <b>[02/2024]</b>
                One paper accepted to CVPR'24. <br/>
            </p>

            <p align=left>
                <b>[10/2023]</b>
                Our Tri-MipRF was selected in ICCV'23 <span style=color:sandybrown>Best Paper Finalist</span>!
                <br/>
            </p>

            <p align=left>
                <b>[08/2023]</b>
                Invited talk by <a href="https://games-cn.org/games-webinar-20230831-290/">GAMES</a> about our<a
                    href="./projects/Tri-MipRF/index.html">Tri-MipRF</a>. <br/>
            </p>

            <p align=left>
                <b>[07/2023]</b>
                Our Tri-MipRF was accepted to ICCV'23 as ORAL presentation. <br/>
            </p>

            <p align=left>
                <b>[07/2023]</b>
                One paper on invertile image downscaling accepted to TIP'23. <br/>
            </p>

            <p align=left>
                <b>[08/2022]</b>
                One paper conditionally accepted to SIGGRAPH Asia'22 (Journal Track). <br/>
            </p>

            <p align=left>
                <b>[05/2022]</b>
                Passed the oral defense and became a Dr. <br/>
            </p>

            <p align=left>
                <b>[08/2021]</b>
                Invited talk by <a href="https://course.zhidx.com/c/MTkzZGQxOGUwNzhiOGE5Njg4ZDM=">Êô∫‰∏úË•ø</a>about our<a
                    href="./projects/BPNet/index.html">BPNet</a>.
                <br/>
            </p>

            <p align=left>
                <b>[07/2021]</b>
                One paper on lighting estimation accepted to TIP'22. <br/>
            </p>

            <p align=left>
                <b>[07/2021]</b>
                Two papers accepted to ICCV'21. <br/>
            </p>

            <p align=left>
                <b>[07/2021]</b>
                One paper on 3D human pose estimation accepted to ACM MM'21. <br/>
            </p>

            <p align=left>
                <b>[06/2021]</b>
                Invited talk at <a href="http://www.scan-net.org/cvpr2021workshop">ScanNet CVPR'21 Workshop </a>about
                our <a
                    href="./projects/BPNet/index.html">BPNet</a> (<a href="https://youtu.be/jr_bKmh6YUY?t=4141">Recording</a>).
                <br/>
            </p>

            <p align=left>
                <b>[06/2021]</b>
                Code of <a href="./projects/BPNet/index.html">BPNet</a> is released now.<br/>
            </p>

            <p align=left>
                <b>[03/2021]</b>
                One paper conditionally accepted to CVPR 2021 (Oral). <br/>
            </p>

            <p align=left>
                <b>[08/2020]</b>
                One paper conditionally accepted to SIGGRAPH Asia 2020. <br/>
            </p>

            <p align=left>
                <b>[08/2018]</b>
                I start my Ph.D study at CUHK. <br/>
            </p>

            <p align=left>
                <b>[01/2018]</b>
                I will work as a research intern at <a href="https://www.sensetime.com/">SenseTime</a>.
            </p>

        </div>

    </div>


    <!--  PUBLICATIONS  -->
    <div class="pubText" id="pub">

        <p class="notation">
            Selected, full list on <a href="https://scholar.google.com/citations?hl=en&user=xDuDhwcAAAAJ">Google
            Scholar</a>
            <br>
            <span style="font-size: medium; margin: 0">(<sup>*</sup>equal contribution, <sup>&dagger;</sup>corresponding author, <sup>‚Ä°</sup>project leader)</span>
        </p>

        <hr class="line"/>

        <div style="max-height: 850px; overflow-y: auto; background-color: #f0f0f0; scrollbar-width: thin;">

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/geometrycrafter.gif"
                        width="380" height="170"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors
                </span>
                    <br>
                    Tian-Xing Xu, Xiangjun Gao, <b>Wenbo Hu<sup>&dagger;</sup></b>, Xiaoyu Li, Song-Hai Zhang<sup>&dagger;</sup>, Ying Shan
                    <br>
                    <em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2025.
                    </br>

                    [ <a href="https://geometrycrafter.github.io/">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/abs/2504.01016">Paper</a> ] &nbsp
                    [ <a href="https://github.com/TencentARC/GeometryCrafter">Code</a> ] &nbsp
                    [ <a href="https://huggingface.co/spaces/TencentARC/GeometryCrafter">Demo</a> ] &nbsp


                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/normalcrafter.gif"
                        width="380" height="195"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    NormalCrafter: Learning Temporally Consistent Normals from Video Diffusion Priors
                </span>
                    <br>
                    Yanrui Bin, <b>Wenbo Hu<sup>‚Ä°</sup></b>, Haoyuan Wang, Xinya Chen, Bing Wang<sup>&dagger;</sup>
                    <br>
                    <em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2025.
                    </br>

                    [ <a href="https://normalcrafter.github.io/">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/abs/2504.11427">Paper</a> ] &nbsp
                    [ <a href="https://github.com/Binyr/NormalCrafter">Code</a> ] &nbsp
                    [ <a href="https://huggingface.co/spaces/Yanrui95/NormalCrafter">Demo</a> ] &nbsp


                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/TrajectoryCrafter.gif"
                        width="380" height="190"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models
                </span>
                    <br>
                    Mark YU, <b>Wenbo Hu<sup>&dagger;</sup></b>, Jinbo Xing, Ying Shan
                    <br>
                    <em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2025.
                    &nbsp <span class="remark"> Oral Presentation</span>
                    </br>

                    [ <a href="https://trajectorycrafter.github.io/">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/pdf/2503.05638">Paper</a> ] &nbsp
                    [ <a href="https://github.com/TrajectoryCrafter/TrajectoryCrafter">Code</a> ] &nbsp
                    [ <a href="https://huggingface.co/spaces/Doubiiu/TrajectoryCrafter ">Demo</a> ] &nbsp


                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/depthcrafter.gif"
                        width="380" height="170"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos
                </span>
                    <br>
                    <b>Wenbo Hu<sup>* &dagger;</sup></b>, Xiangjun Gao<sup>*</sup>, Xiaoyu Li<sup>* &dagger;</sup>,
                    Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan
                    <br>
                    <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2025.
                    &nbsp <span class="remark"> Highlight</span>
                    </br>
                    [ <a href="https://depthcrafter.github.io/">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/abs/2409.02095">Paper</a> ] &nbsp
                    [ <a href="https://github.com/wbhu/DepthCrafter">Code</a> ] &nbsp
                    [ <a href="https://huggingface.co/spaces/tencent/DepthCrafter">Demo</a> ] &nbsp
                    <span class="remark"> Best Paper at PixFoundation workshop</span>

                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/viewcrafter.gif"
                        width="380" height="170"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis
                </span>
                    <br>
                    Wangbo Yu<sup>*</sup>, Jinbo Xing<sup>*</sup>, Li Yuan<sup>*</sup>, <b>Wenbo
                    Hu<sup>&dagger;</sup></b>, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan,
                    Yonghong Tian<sup>&dagger;</sup>
                    <br>
                    <span class="remark"> TPAMI 2025</span>
                    </br>
                    [ <a href="https://drexubery.github.io/ViewCrafter">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/abs/2409.02048">Paper</a> ] &nbsp
                    [ <a href="https://www.youtube.com/watch?v=WGIEmu9eXmU">Video</a> ] &nbsp
                    [ <a href="https://github.com/Drexubery/ViewCrafter">Code</a> ] &nbsp
                    [ <a href="https://huggingface.co/spaces/Doubiiu/ViewCrafter">Demo</a> ] &nbsp
                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/stereocrafter.png"
                        width="380" height="170"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    StereoCrafter: Diffusion-based Generation of Long and High-fidelity Stereoscopic 3D from Monocular Videos
                </span>
                    <br>
                    Sijie Zhao<sup>*</sup>, <b>Wenbo Hu<sup>*</sup></b>, Xiaodong Cun<sup>*</sup>, Yong Zhang<sup>&dagger;</sup>,
                    Xiaoyu Li<sup>&dagger;</sup>, Zhe Kong, Xiangjun Gao, Muyao Niu, Ying Shan
                    <br>
                    <em>arXiv preprint</em>, 2024.
                    <!--</br>-->
                    [ <a href="https://stereocrafter.github.io/">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/abs/2409.07447">Paper</a> ] &nbsp
                    [ <a href="https://github.com/TencentARC/StereoCrafter">Code</a> ] &nbsp
                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/pixel-gs.png"
                        width="380" height="150"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting
                </span>
                    <br>
                    Zheng Zhang, <b>Wenbo Hu<sup>&dagger;</sup></b>, Yixing Lao, Tong He, Hengshuang
                    Zhao<sup>&dagger;</sup>
                    <br>
                    <em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2024.
                    </br>
                    [ <a href="https://pixelgs.github.io">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/pdf/2403.15530.pdf">Paper</a> ] &nbsp
                    [ <a href="https://github.com/zhengzhang01/Pixel-GS">Code</a> ] &nbsp
                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/analytic-splatting.jpg"
                        width="380" height="150"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration
                </span>
                    <br>
                    Zhihao Liang, Qi Zhang, <b>Wenbo Hu</b>, Lei Zhu, Ying Feng, Kui Jia
                    <br>
                    <em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2024.
                    &nbsp <span class="remark"> Oral Presentation</span>
                    </br>
                    [ <a href="https://lzhnb.github.io/project-pages/analytic-splatting">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/pdf/2403.11056.pdf">Paper</a> ] &nbsp
                    [ <a href="https://github.com/lzhnb/Analytic-Splatting">Code</a> ] &nbsp
                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/texture-gs.jpg"
                        width="380" height="150"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing
                </span>
                    <br>
                    Tian-Xing Xu, <b>Wenbo Hu<sup>&dagger;</sup></b>, Yu-Kun Lai, Ying Shan, Song-Hai
                    Zhang<sup>&dagger;</sup>
                    <br>
                    <em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2024.
                    </br>
                    [ <a href="https://slothfulxtx.github.io/TexGS">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/abs/2403.10050">Paper</a> ] &nbsp
                    [ <a href="https://github.com/slothfulxtx/Texture-GS">Code</a> ] &nbsp
                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/ripnerf.jpg"
                        width="380" height="150"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic Solids
                </span>
                    <br>
                    Junchen Liu<sup>*</sup>, <b>Wenbo Hu<sup>*</sup></b>, Zhuo Yang<sup>*</sup>, Jianteng Chen, Guoliang
                    Wang,
                    Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao
                    <br>
                    <span class="remark"> SIGGRAPH 2024</span>
                    </br>
                    [ <a href="https://junchenliu77.github.io/Rip-NeRF/">Project</a> ] &nbsp
                    [ <a href="https://junchenliu77.github.io/Rip-NeRF/resources/Rip-NeRF.pdf">Paper</a> ] &nbsp
                    [ <a href="https://github.com/JunchenLiu77/Rip-NeRF">Code</a> ] &nbsp
                    [ <a href="https://drive.google.com/file/d/1lmLJ7VN-Fbyohgdcrl7WMDj3gPk3WgUg">Data</a> ] &nbsp
                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/pbr-nerf.jpg"
                        width="380" height="150"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Inverse Rendering of Glossy Objects via the Neural Plenoptic Function and Radiance Fields
                </span>
                    <br>
                    Haoyuan Wang, <b>Wenbo Hu<sup>&dagger;</sup></b>, Lei Zhu, Rynson W.H. Lau<sup>&dagger;</sup>
                    <br>
                    <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024.
                    </br>
                    [ <a href="https://www.whyy.site/paper/nep">Project</a> ] &nbsp
                    [ <a href="pdf/PBR-NeRF.pdf">Paper</a> ] &nbsp
                    [ <a href="https://github.com/onpix/NeP">Code</a> ] &nbsp
                    [ <a href="https://www.whyy.site/paper/nep">Data</a> ] &nbsp
                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/trimip.jpg"
                        width="380" height="150"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields
                </span>
                    <br>
                    <b>Wenbo Hu</b>, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, Yuewen Ma
                    <br>
                    <em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2023.
                    &nbsp <span class="remark"> Oral Presentation</span>
                    </br>
                    [ <a href="projects/Tri-MipRF/index.html">Project</a> ] &nbsp
                    [ <a href="pdf/Tri-MipRF.pdf">Paper</a> ] &nbsp
                    [ <a href="https://github.com/wbhu/Tri-MipRF">Code</a> ] &nbsp
                    <span class="remark">
                    Best paper finalist (17/8260)
                </span>
                </div>
            </div>


            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/aidn.gif"
                        width="380" height="150"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Scale-arbitrary Invertible Image Downscaling
                </span>
                    <br>
                    Jinbo Xing<sup>*</sup>, <b>Wenbo Hu<sup>*</sup></b>, Menghan Xia, Tien-Tsin Wong
                    <br>
                    <em>IEEE Transactions on Image Processing</em> (<b>TIP</b>), 2023.
                    </br>
                    [ <a href="https://doubiiu.github.io/projects/aidn/">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/abs/2201.12576">Paper</a> ] &nbsp
                    [ <a href="https://github.com/Doubiiu/AIDN">Code</a> ] &nbsp
                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/disColor.jpeg"
                        width="380" height="145"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Disentangled Image Colorization via Global Anchors
                </span>
                    <br>
                    Menghan Xia, <b>Wenbo Hu</b>, Tien-Tsin Wong, Jue Wang
                    <br>
                    <em>ACM Transactions on Graphics</em> (<b>TOG</b>), 2022.
                    &nbsp <span class="remark"> SIGGRAPH Asia Journal Track</span>

                    </br>
                    [ <a href="https://menghanxia.github.io/projects/disco.html">Project</a> ] &nbsp
                    [ <a href="https://menghanxia.github.io/projects/disco/disco_main.pdf">Paper</a> ] &nbsp
                    [ <a href="https://github.com/MenghanXia/DisentangledColorization">Code</a> ] &nbsp

                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/invHalftone.jpeg"
                        width="380" height="120"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Deep Halftoning with Reversible Binary Pattern
                </span>
                    <br>
                    Menghan Xia, <b>Wenbo Hu</b>, Xueting Liu, Tien-Tsin Wong
                    <br>
                    <em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2021.
                    </br>
                    [ <a href="https://www.cse.cuhk.edu.hk/~ttwong/papers/invhalftone/invhalftone.html">Project</a> ]
                    &nbsp
                    [ <a
                        href="http://appsrv.cse.cuhk.edu.hk/~ttwong/cgi-bin/paper-download/download.cgi?path=invhalftone&dl=invhalftone.pdf">Paper</a>
                    ] &nbsp
                    [ <a href="https://github.com/MenghanXia/ReversibleHalftoning">Code</a> ] &nbsp

                </div>
            </div>

            <div class="post-container">
                <div class="post-thumb"><img
                        src="img/CondDGConv.jpg"
                        width="380" height="130"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Conditional Directed Graph Convolution for 3D Human Pose Estimation
                </span>
                    <br>
                    <b>Wenbo Hu</b>, Changgong Zhang, Fangneng Zhan, Lei Zhang, Tien-Tsin Wong
                    <br>
                    <em>ACM International Conference on Multimedia</em> (<b>ACM MM</b>), 2021.
                    <br>
                    [ <a href="projects/CondDGConv/index.html">Project</a> ] &nbsp
                    [ <a href="./pdf/CondDGConv_update.pdf">Paper</a> ] &nbsp
                </div>
            </div>


            <div class="post-container">
                <div class="post-thumb"><img
                        src="projects/BPNet/img/bpnet.jpg"
                        width="380" height="140"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Bidirectional Projection Network for Cross Dimensional Scene Understanding
                </span>
                    <br>
                    <b>Wenbo Hu</b>, Hengshuang Zhao, Li Jiang, Jiaya Jia, Tien-Tsin Wong
                    <br>
                    <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2021.
                    &nbsp <span class="remark"> Oral Presentation</span>
                    </br>
                    [ <a href="projects/BPNet/index.html">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/abs/2103.14326">Paper</a> ] &nbsp
                    [ <a href="https://github.com/wbhu/BPNet">Code</a> ] &nbsp
                    <!--                <br>-->
                    <!--                <span style="font-size: large; "><b><span style=color:red>Top performance on ScanNet 3D/2D semantic segmentation benchmarks!</span></b></span>-->
                    <!--                [<a href="http://kaldir.vc.in.tum.de/scannet_benchmark/semantic_label_3d">Link</a> ]-->
                </div>
            </div>


            <div class="post-container">
                <div class="post-thumb"><img
                        src="projects/Mono3D/img/teaser.jpg"
                        width="380" height="140"/>
                </div>
                <div class="post-content">
                <span class="paper-title">
                    Mononizing Binocular Videos
                </span>
                    <br>
                    <b>Wenbo Hu</b>, Menghan Xia, Chi-Wing Fu, Tien-Tsin Wong
                    <br>
                    <em>ACM Transactions on Graphics</em> (<b>TOG</b>), 2020.
                    &nbsp <span class="remark"> SIGGRAPH Asia</span>
                    </br>
                    [ <a href="projects/Mono3D/index.html">Project</a> ] &nbsp
                    [ <a href="https://arxiv.org/abs/2009.01424">Paper</a> ] &nbsp
                    [ <a href="https://github.com/wbhu/Mono3D">Code</a> ] &nbsp
                </div>
            </div>

            <!--        <div class="post-container">-->
            <!--            <div class="post-thumb"><img-->
            <!--                    src="img/vec.jpg"-->
            <!--                    width="380" height="140"/>-->
            <!--            </div>-->
            <!--            <div class="post-content">-->
            <!--                <b>Deep Line Drawing Vectorization via Line Subdivision and Topology Reconstruction</b>-->
            <!--                <br>-->
            <!--                &nbsp-->
            <!--                Yi Guo, Zhuming Zhang, Chu Han, <I>Wenbo Hu,</I> Chengze Li, Tien-Tsin Wong-->
            <!--                <br>-->
            <!--                <I>Computer Graphics Forum (CGF), <b>Pacific Graphics (PG),</b> 2019</I>-->
            <!--                <br>-->
            <!--                &nbsp-->
            <!--                [<a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/cgf.13818"> Paper</a>]-->
            <!--            </div>-->
            <!--        </div>-->

            <!--        <div class="post-container">-->
            <!--            <div class="post-thumb"><img-->
            <!--                    src="img/DEMC.jpg"-->
            <!--                    width="380" height="140"/>-->
            <!--            </div>-->
            <!--            <div class="post-content">-->
            <!--                <b>DEMC: A Deep Dual-Encoder Network for Denoising Monte Carlo Rendering</b>-->
            <!--                <br>-->
            <!--                &nbsp Xin Yang, <I>Wenbo Hu,</I> Dawei Wang, Lijing Zhao, Baocai Yin, Qiang Zhang,-->
            <!--                <br>-->
            <!--                &nbsp-->
            <!--                Xiaopeng Wei, Hongbo Fu-->
            <!--                <br>-->
            <!--                <em>Computational Visual Media,</em><b>CVM,</b>2019.-->
            <!--                </br>-->
            <!--                &nbsp-->
            <!--                [<a href="https://sites.google.com/view/dutdemc"> Project</a>]-->
            <!--                [<a href="https://arxiv.org/pdf/1905.03908.pdf"> Paper</a>]-->
            <!--                &lt;!&ndash;                [<a href="bib/DEMC.bib"> Bib</a> ]&ndash;&gt;-->
            <!--                [<a href="https://drive.google.com/drive/folders/1sBDN5vHykSB-YNU_pvzzV1u5WLLgw7X-?usp=sharing">-->
            <!--                Dataset</a> ]-->
            <!--                [<a href="https://github.com/wbhu/DEMC"> Code</a> ]-->

            <!--            </div>-->
            <!--        </div>-->

            <!--        <div class="post-container">-->
            <!--            <div class="post-thumb"><img-->
            <!--                    src="img/FRMC.gif"-->
            <!--                    width="380" height="140"/>-->
            <!--            </div>-->
            <!--            <div class="post-content">-->
            <!--                <b>Fast Reconstruction for Monte Carlo Rendering Using Deep Convolutional Networks</b>-->
            <!--                <br>-->
            <!--                &nbsp-->
            <!--                Xin Yang, Dawei Wang, <I>Wenbo Hu,</I> Lijing Zhao, Xinglin Piao, Dongsheng Zhou,-->
            <!--                <br>-->
            <!--                &nbsp-->
            <!--                Qiang Zhang, Baocai Yin, Qiang Cai, Xiaopeng Wei-->
            <!--                <br>-->
            <!--                <I>IEEE Access, 2018</I>-->
            <!--                <br>-->
            <!--                &nbsp-->
            <!--                [<a href="https://ieeexplore.ieee.org/abstract/document/8571234"> Paper</a>]-->
            <!--                &lt;!&ndash;                [<a href="bib/FRMC.bib"> Bib</a> ]&ndash;&gt;-->
            <!--            </div>-->
            <!--        </div>-->
        </div>


    </div>


    <!--  EXPERIENCE  -->
    <div class="mainText" id="exp">
        <h3>Education</h3>
        <hr class="line"/>

        <div class="post-container">
            <div class="post-thumb"><img
                    src="img/cuhk_logo.jpeg" width="500"/>
            </div>
            <div class="post-content">
                <p><b>Aug. 2018 - May. 2022</b></p>
                <p>
                    &nbsp
                    The Chinese University of Hong Kong
                    <br>
                    &nbsp
                    Dept of Computer Science & Engineering
                    <br>
                    <em>Ph.D.</em>
                </p>
            </div>
        </div>

        <div class="post-container">
            <div class="post-thumb"><img
                    src="img/dut_long.jpeg" width="500"/>
            </div>
            <div class="post-content">
                <p><b>Sep. 2014 - Jun. 2018</b></p>
                <p>
                    &nbsp
                    Dalian University of Technology
                    <br>
                    &nbsp
                    Dept of Computer Science and Technology
                    <br>
                    <em>Bachelor</em> (Rank: 3/105, GPA: 90.3/100)
                </p>
            </div>
        </div>

        <br><br><br><br>
        <h3>Work Experience</h3>
        <hr class="line"/>

        <div class="post-container">
            <div class="post-thumb"><img
                    src="img/TencentAILab.jpeg" width="350"/>
            </div>
            <div class="post-content">
                <p><b>Dec. 2023 - Present</b></p>
                <p>
                    <a href="https://ai.tencent.com">Tencent AI Lab</a>
                    <br>
                    <em>Researcher at CVC</em>
                </p>
            </div>
        </div>

        <div class="post-container">
            <div class="post-thumb"><img
                    src="img/PICO.jpg" width="350"/>
            </div>
            <div class="post-content">
                <p><b>Sep. 2022 - Nov. 2023</b></p>
                <p>
                    <a href="https://www.picoxr.com/global">PICO Mixed Reality, ByteDance</a>
                    <br>
                    <em>Researcher & Developer</em>
                </p>
            </div>
        </div>

        <div class="post-container">
            <div class="post-thumb"><img
                    src="img/TencentAILab.jpeg" width="350"/>
            </div>
            <div class="post-content">
                <p><b>Nov. 2021 - May. 2022</b></p>
                <p>
                    <a href="https://ai.tencent.com">Tencent AI Lab</a>
                    <br>
                    <em>Research Intern</em>
                </p>
            </div>
        </div>

        <div class="post-container">
            <div class="post-thumb"><img
                    src="img/DAMO.jpeg" width="350"/>
            </div>
            <div class="post-content">
                <p><b>Sep. 2020 - Nov. 2021</b></p>
                <p>
                    <a href="https://damo.alibaba.com/">DAMO Academy</a>, Alibaba Group
                    <br>
                    <em>Research Intern</em>
                </p>
            </div>
        </div>

        <div class="post-container">
            <div class="post-thumb"><img
                    src="img/sensetime.jpg" width="350"/>
            </div>
            <div class="post-content">
                <p><b>Jan. 2018 - Jul. 2018</b></p>
                <p>
                    <!--                    &nbsp-->
                    <a href="https://www.sensetime.com/">SenseTime</a>
                    <br>
                    <em>Research Intern</em>
                </p>
            </div>
        </div>

        <div class="post-container">
            <div class="post-thumb"><img
                    src="img/dji.jpg" width="350"/>
            </div>
            <div class="post-content">
                <p><b>Jul. 2016 - Sep. 2016</b></p>
                <p>
                    <!--                    &nbsp-->
                    <a href="https://www.dji.com/">DJI</a>
                    <br>
                    <em>Summer Intern</em>
                </p>
            </div>
        </div>

    </div>

    <!--  PATENTS  -->
    <div class="mainText" id="pat">


        <ul>
            <li>CN108827302A (In process), ‚ÄúMulti-rotor aerocraft air navigation aid based on rotor tachometric
                survey‚Äù.
            </li>
            <li>CN107656227B (Issued Oct. 2019), ‚ÄúMagnetometer calibration method based on Levenberg-Marquardt
                algorithm‚Äù.
            </li>
            <li>CN107655463B (Issued Oct. 2019), ‚ÄúElectronic compass calibration method based on simulated annealing‚Äù.
            </li>
            <li>CN207400517U (Issued May. 2018), ‚ÄúIntelligent toothbrush system ‚Äù.</li>
        </ul>
    </div>

    <!--  Miscellaneous -->
    <div class="mainText" id="mis">
        <h3>Miscellaneous</h3>
        <hr class="line"/>

        <!--<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share our <a-->
        <!--        href="https://twitter.com/hashtag/TrajectoryCrafter?src=hash&amp;ref_src=twsrc%5Etfw">#TrajectoryCrafter</a>,-->
        <!--    a diffusion model for Redirecting Camera Trajectory in Monocular Videos! <br><br>Try to explore the world-->
        <!--    underlying your videos~<br><br>Page: <a href="https://t.co/hWuDRDcv10">https://t.co/hWuDRDcv10</a><br>Demo:-->
        <!--    <a href="https://t.co/e3JF0SSXIC">https://t.co/e3JF0SSXIC</a><br>Code: <a href="https://t.co/Y84MN6D2iO">https://t.co/Y84MN6D2iO</a>-->
        <!--    <a href="https://t.co/WAS9Vbbosu">pic.twitter.com/WAS9Vbbosu</a></p>&mdash; HU, Wenbo (@wbhu_cuhk) <a-->
        <!--        href="https://twitter.com/wbhu_cuhk/status/1898988354143674423?ref_src=twsrc%5Etfw">March 10, 2025</a>-->
        <!--</blockquote>-->
        <!--<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>-->

        <!--<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share our DepthCrafter, a super consistent-->
        <!--    video depth model for long open-world videos!<br><br>Project webpage: <a href="https://t.co/9SiMUv4hoW">https://t.co/9SiMUv4hoW</a>-->
        <!--    <a href="https://t.co/qy55L7cm44">https://t.co/qy55L7cm44</a> <a href="https://t.co/TZBWnjEGmk">pic.twitter.com/TZBWnjEGmk</a>-->
        <!--</p>&mdash; HU, Wenbo (@wbhu_cuhk) <a-->
        <!--        href="https://twitter.com/wbhu_cuhk/status/1831227896331112591?ref_src=twsrc%5Etfw">September 4,-->
        <!--    2024</a></blockquote>-->
        <!--<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>-->

        <div class="tweet-container">

            <div class="tweet-item">
                <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share our <a
                        href="https://twitter.com/hashtag/TrajectoryCrafter?src=hash&amp;ref_src=twsrc%5Etfw">#TrajectoryCrafter</a>,
                    a diffusion model for Redirecting Camera Trajectory in Monocular Videos! <br><br>Try to explore the
                    world
                    underlying your videos~<br><br>Page: <a
                            href="https://t.co/hWuDRDcv10">https://t.co/hWuDRDcv10</a><br>Demo:
                    <a href="https://t.co/e3JF0SSXIC">https://t.co/e3JF0SSXIC</a><br>Code: <a
                            href="https://t.co/Y84MN6D2iO">https://t.co/Y84MN6D2iO</a>
                    <a href="https://t.co/WAS9Vbbosu">pic.twitter.com/WAS9Vbbosu</a></p>&mdash; HU, Wenbo (@wbhu_cuhk)
                    <a
                            href="https://twitter.com/wbhu_cuhk/status/1898988354143674423?ref_src=twsrc%5Etfw">March
                        10, 2025</a>
                </blockquote>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
            </div>

            <div class="tweet-item">
                <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Introducing ùöÖùöíùöéùö†ùô≤ùöõùöäùöèùöùùöéùöõ ü•≥.
                    ùöÖùöíùöéùö†ùô≤ùöõùöäùöèùöùùöéùöõ can generate high-fidelity novel views from single or sparse input images with accurate
                    camera pose control!<br>‚ú®Paper: <a href="https://t.co/dH4Dw0Eb1e">https://t.co/dH4Dw0Eb1e</a><br>üéØCode:
                    <a href="https://t.co/53ai21Px99">https://t.co/53ai21Px99</a><br>ü•ÅDemo: <a
                            href="https://t.co/9xCyqtsFco">https://t.co/9xCyqtsFco</a> <a
                            href="https://t.co/R5ZZpYfdM3">pic.twitter.com/R5ZZpYfdM3</a></p>&mdash; Jinbo Xing
                    (@Double47685693) <a
                            href="https://twitter.com/Double47685693/status/1831517092606308703?ref_src=twsrc%5Etfw">September
                        5, 2024</a></blockquote>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
            </div>

            <div class="tweet-item">
                <blockquote class="twitter-tweet"><p lang="en" dir="ltr">I ported DepthCrafter to ComfyUI! üî•<br><br>Now
                    you can generate super stable depthmap videos from any input video.<br><br>The VRAM requirement is
                    pretty high (&gt;16GB) if you want to render long videos in high res (768p and up)<br><br>It pairs
                    well with Depthflow!<br><br>Repo link in comments belowüëá <a href="https://t.co/dlS5wr7mRU">pic.twitter.com/dlS5wr7mRU</a>
                </p>&mdash; akatz (@akatz_ai) <a
                        href="https://twitter.com/akatz_ai/status/1847374687384527209?ref_src=twsrc%5Etfw">October 18,
                    2024</a></blockquote>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
            </div>

            <div class="tweet-item">
                <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Introducing StereoCrafter: Transforming
                    monocular videos into high-fidelity 3D movies, compatible with various depth estimation methods and
                    currently performing best with DepthCrafter. Feel free to download and experience the 3D results on
                    our project page using Vision Pros,‚Ä¶ <a
                            href="https://t.co/6YEHisStz8">pic.twitter.com/6YEHisStz8</a></p>&mdash; Ying Shan
                    (@yshan2u) <a href="https://twitter.com/yshan2u/status/1834126260341277024?ref_src=twsrc%5Etfw">September
                        12, 2024</a></blockquote>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
            </div>

            <div class="tweet-item">
                <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share our DepthCrafter, a super
                    consistent
                    video depth model for long open-world videos!<br><br>Project webpage: <a
                            href="https://t.co/9SiMUv4hoW">https://t.co/9SiMUv4hoW</a>
                    <a href="https://t.co/qy55L7cm44">https://t.co/qy55L7cm44</a> <a href="https://t.co/TZBWnjEGmk">pic.twitter.com/TZBWnjEGmk</a>
                </p>&mdash; HU, Wenbo (@wbhu_cuhk) <a
                        href="https://twitter.com/wbhu_cuhk/status/1831227896331112591?ref_src=twsrc%5Etfw">September 4,
                    2024</a></blockquote>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
            </div>

            <div class="tweet-item">
                <blockquote class="twitter-tweet"><p lang="zxx" dir="ltr"><a href="https://t.co/ZzpcYAHw6I">pic.twitter.com/ZzpcYAHw6I</a>
                </p>&mdash; Kosta Derpanis (@CSProfKGD) <a
                        href="https://twitter.com/CSProfKGD/status/1709468280371466453?ref_src=twsrc%5Etfw">October 4,
                    2023</a></blockquote>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
            </div>

        </div>


    </div>

    <script type="text/javascript" src="./script.js"></script>

</body>
</html>
